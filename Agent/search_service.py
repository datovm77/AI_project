import requests
import json
import streamlit as st
import re
import time
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed

## python search_service.py
## è”ç½‘æœç´¢æ•°æ®å¤„ç†æ•°æ®aiæ€»ç»“æ•°æ®è¿”å›æ•°æ®æ¨¡å—

# === é…ç½®åŒºåŸŸ ===
# å±è”½åˆ—è¡¨ï¼šè·³è¿‡è¿™äº›æ— æ³•æŠ“å–æˆ–æ— å…³çš„ç½‘ç«™
BLOCKED_SITES = [
    "youtube.com", "youtu.be",
    "twitter.com", "x.com",
    "facebook.com", "instagram.com",
    "linkedin.com", "pinterest.com",
    "tiktok.com", "douyin.com",
    "bilibili.com" # è§†é¢‘ç«™ç‚¹é€šå¸¸åªæœ‰å­—å¹•ï¼Œä¸”å®¹æ˜“è¶…æ—¶
]

# è¯·æ±‚å¤´ï¼šæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œé¿å…è¢«åçˆ¬
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
}

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=st.secrets["API_KEY"] 
)
MODEL_NAME = "x-ai/grok-4.1-fast"

def ai_extract_json(content, url, max_retries=2):
    """
    å°†æ‚ä¹±çš„ç½‘é¡µæ–‡æœ¬æ¸…æ´—ä¸ºä¸¥æ ¼çš„ JSON æ ¼å¼ (å¸¦é‡è¯•æœºåˆ¶)
    """
    
    # ã€ä¿é™© 2ã€‘System Promptï¼šæå…¶ä¸¥æ ¼çš„çº¦æŸ
    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªä¸çŸ¥ç–²å€¦çš„æ•°æ®æå–APIã€‚
    ä»»åŠ¡ï¼šé˜…è¯»ç”¨æˆ·æä¾›çš„ç½‘é¡µæ–‡æœ¬ï¼Œæå–ä¿¡æ¯ã€‚
    
    ã€é‡è¦æç¤ºã€‘
    1. ç½‘é¡µæ–‡æœ¬å¯èƒ½åŒ…å«å¤§é‡å¯¼èˆªèœå•ã€å¹¿å‘Šæˆ–æ— å…³é“¾æ¥ï¼Œè¯·å¿½ç•¥å®ƒä»¬ï¼Œåªå…³æ³¨æ ¸å¿ƒæ­£æ–‡ã€‚
    2. å³ä½¿æ­£æ–‡è¢«å¤§é‡å¯¼èˆªåŒ…è£¹ï¼Œåªè¦èƒ½æ‰¾åˆ°æœ‰ä»·å€¼çš„å†…å®¹ï¼Œå°±è§†ä¸ºæœ‰æ•ˆã€‚

    ã€ä¸¥æ ¼è¾“å‡ºçº¦æŸã€‘
    1. ä½ å¿…é¡»åªè¾“å‡º RFC8259 æ ‡å‡†çš„ JSON å­—ç¬¦ä¸²ã€‚
    2. ä¸è¦ä½¿ç”¨ Markdown ä»£ç å—ï¼ˆå³ä¸è¦ç”¨ ```json å¼€å¤´ï¼‰ã€‚
    3. å¦‚æœç½‘é¡µå†…å®¹æ— æ•ˆï¼ˆå¦‚å…¨æ˜¯ä¹±ç ã€éªŒè¯ç ã€ç™»å½•é¡µï¼‰ï¼Œè¯·å°† "valid" å­—æ®µè®¾ä¸º falseã€‚
    
    ã€è¾“å‡º JSON æ¨¡ç‰ˆã€‘
    {
        "valid": true,
        "title": "ç½‘é¡µæ ‡é¢˜",
        "summary": "500å­—ä»¥å†…çš„æ€»ç»“ï¼Œæå–æ ¸å¿ƒå†…å®¹ï¼Œæ˜¯ä»€ä¹ˆç±»å‹æç‚¼å‡ºä»€ä¹ˆç±»å‹",
        "key_points": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2", "å…³é”®ç‚¹3"],
        "code_snippets": ["æå–åˆ°çš„å…³é”®ä»£ç ç‰‡æ®µ(å¦‚æœæœ‰)"],
        "source_url": "åŸé“¾æ¥"
    }
    """

    # æˆªæ–­è¿‡é•¿å†…å®¹ï¼Œé˜²æ­¢ Token æº¢å‡ºæˆ–è´¹ç”¨è¿‡é«˜
    user_prompt = f"åŸæ–‡é“¾æ¥: {url}\n\nåŸæ–‡å†…å®¹:\n{content[:60000]}" 

    for attempt in range(max_retries + 1):
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"}, 
                temperature=0.1, 
                timeout=45 
            )
            
            raw_content = response.choices[0].message.content
            result = clean_and_parse_json(raw_content)
            
            if result:
                # å†æ¬¡ç¡®è®¤ valid å­—æ®µ
                if not result.get("valid", True):
                    print(f"   [AIåˆ¤å®šæ— æ•ˆ]: {url}")
                    return None
                return result
                
        except Exception as e:
            print(f"   [AIæ€»ç»“é‡è¯• {attempt+1}/{max_retries+1}] {e}")
            time.sleep(1) # é¿è®©ä¸€ä¸‹
            
    return None

def clean_and_parse_json(raw_text):
    """
    ã€ä¿é™© 3ã€‘Python ä»£ç æ¸…æ´—ï¼šé˜²æ­¢ AI åŠ äº† ```json å¯¼è‡´è§£æå¤±è´¥
    """
    try:
        # 1. å»æ‰å¯èƒ½å­˜åœ¨çš„ Markdown æ ‡è®°
        text = re.sub(r'<think>.*?</think>', '', raw_text, flags=re.DOTALL)
        text = re.sub(r'^```json\s*', '', text, flags=re.MULTILINE) 
        text = re.sub(r'^```\s*', '', text, flags=re.MULTILINE)
        text = text.strip()
        
        # 2. å°è¯•è§£æ
        return json.loads(text)
    except json.JSONDecodeError:
        print("   [è§£æå¤±è´¥] JSON æ ¼å¼é”™è¯¯")
        return None


def _clean_html(raw_html):
    """
    ç®€å•æ¸…æ´— HTMLï¼Œç§»é™¤ script/style å’Œæ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬
    """
    # 1. ç§»é™¤ script å’Œ style
    text = re.sub(r'<script.*?>.*?</script>', '', raw_html, flags=re.DOTALL)
    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)
    
    # 2. ç§»é™¤æ³¨é‡Š
    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)
    
    # 3. ç§»é™¤ HTML æ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    
    # 4. å¤„ç†å¤šä½™ç©ºè¡Œ
    text = re.sub(r'\n\s*\n', '\n\n', text)
    
    return text.strip()

def fetch_jina_content(link, max_retries=2):
    """
    å°è¯•ä½¿ç”¨ Jina Reader æŠ“å–å†…å®¹ã€‚
    å¦‚æœ Jina å¤±è´¥ (403/404ç­‰)ï¼Œåˆ™å›é€€åˆ°ç›´æ¥ requests æŠ“å– + ç®€å•æ­£åˆ™æ¸…æ´—ã€‚
    """
    jina_url = f"https://r.jina.ai/{link}"
    
    # --- é˜¶æ®µ 1: å°è¯• Jina ---
    for attempt in range(max_retries + 1):
        try:
            read_res = requests.get(jina_url, headers=HEADERS, timeout=(10, 30))
            
            if read_res.status_code == 200:
                content = read_res.text
                # Jina è¿”å›çš„æ˜¯ Markdownï¼Œåšç®€å•æ¸…æ´—
                content_clean = re.sub(r'!\[.*?\]\(.*?\)', '', content)
                content_clean = re.sub(r'\n\s*\n', '\n\n', content_clean)
                return content_clean
                
            elif read_res.status_code in [429, 500, 502, 503, 504]:
                time.sleep(1 * (attempt + 1))
                continue
            else:
                # 403/404 ç­‰é”™è¯¯ï¼Œç›´æ¥è·³å‡º Jina é‡è¯•å¾ªç¯ï¼Œè¿›å…¥ fallback
                print(f"   [Jinaå¤±è´¥ {read_res.status_code}]ï¼Œå°è¯•ç›´è¿: {link}")
                break
                
        except Exception as e:
            if attempt < max_retries:
                time.sleep(1)
                continue
            print(f"   [Jinaå‡ºé”™]: {str(e)[:50]}")
            
    # --- é˜¶æ®µ 2: Fallback ç›´è¿æŠ“å– ---
    print(f"   [Fallback] ç›´è¿æŠ“å–: {link}")
    try:
        # ç›´è¿ä¹Ÿå°è¯• 2 æ¬¡
        for attempt in range(2):
            try:
                # æ·»åŠ  Referer å¯èƒ½æœ‰åŠ©äºé€šè¿‡éƒ¨åˆ†åçˆ¬
                direct_headers = HEADERS.copy()
                direct_headers["Referer"] = "https://www.google.com/"
                
                res = requests.get(link, headers=direct_headers, timeout=(10, 30))
                

                if res.status_code == 200:
                    # è§£å†³ä¹±ç 
                    res.encoding = res.apparent_encoding
                    
                    # åªæœ‰æ–‡æœ¬è¶³å¤Ÿé•¿æ‰è§†ä¸ºæœ‰æ•ˆ
                    if len(res.text) < 500:
                        print(f"   [ç›´è¿] å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½è¢«éªŒè¯æ‹¦æˆª: {link}")
                        continue
                        
                    # æ¸…æ´— HTML
                    cleaned_text = _clean_html(res.text)
                    return cleaned_text
                    
                elif res.status_code == 403:
                    print(f"   [ç›´è¿ 403] ä¾ç„¶è¢«æ‹¦æˆª: {link}")
                    # 403 é€šå¸¸é‡è¯•ä¹Ÿæ²¡ç”¨ï¼Œé™¤éæ¢ IPï¼Œè¿™é‡Œç›´æ¥æ”¾å¼ƒ
                    break
                else:
                    time.sleep(1)
            except Exception as e:
                 print(f"   [ç›´è¿å¼‚å¸¸]: {e}")
                 time.sleep(1)

    except Exception as e:
        print(f"   [Fallbackå¤±è´¥]: {e}")

    return None


def process_single_search_result(idx, item):
    """
    å¤„ç†å•ä¸ªæœç´¢ç»“æœçš„å·¥ä½œå•å…ƒ (Thread Worker)
    """
    title = item.get('title')
    link = item.get('link')
    snippet = item.get('snippet')
    
    # 1. å±è”½æ£€æŸ¥
    if any(blocked in link for blocked in BLOCKED_SITES):
        print(f"[{idx+1}] è·³è¿‡å±è”½ç½‘ç«™: {link}")
        return None

    print(f"[{idx+1}] å¼€å§‹å¤„ç†: {title[:20]}...")

    # 2. æŠ“å–æ­£æ–‡
    content = fetch_jina_content(link)
    
    if not content:
        return None
        
    if len(content) < 300:
        print(f"   [å†…å®¹æ— æ•ˆ] è¿‡çŸ­({len(content)}å­—): {link}")
        return None
        
    print(f"   [æŠ“å–æˆåŠŸ] ({len(content)}å­—)ï¼Œæ­£åœ¨AIæ€»ç»“...")

    # 3. AI æ€»ç»“
    structured_data = ai_extract_json(content, link)
    
    if structured_data:
        print(f"   [âœ… å¤„ç†å®Œæˆ] {title[:15]}...")
        # è¡¥å…¨æŸäº›å­—æ®µé˜²ä¸¢å¤±
        if "source_url" not in structured_data:
            structured_data["source_url"] = link
        return structured_data
    else:
        print(f"   [âŒ æ€»ç»“å¤±è´¥] {title[:15]}...")
        return None

def search_for_keyword(query:str):
    url = "https://google.serper.dev/search"
    
    try:
        api_key_search = st.secrets["API_SEARCH"]
    except:
        print(" æ²¡æ‰¾åˆ° st.secretsï¼Œè¯·ç›´æ¥åœ¨ä»£ç é‡Œå¡«å…¥ Key æµ‹è¯•")
        return []
    
    print(f"ğŸš€ æ­£åœ¨å¹¶å‘æœç´¢: {query} ...")

    payload = json.dumps({
        "q": query,
        "gl": "cn",
        "hl": "zh-cn",
        "num": 3 
    })
    
    headers = {
        'X-API-KEY': api_key_search,
        'Content-Type': 'application/json'
    }

    try:
        response = requests.request("POST", url, headers=headers, data=payload, timeout=(5, 30))
        search_data = response.json()
    except Exception as e:
        print(f"æœç´¢ API è¯·æ±‚å¤±è´¥: {e}")
        return []

    final_results = []
    
    if "organic" in search_data:
        results_list = search_data["organic"]
        print(f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results_list)} ä¸ªåŸå§‹ç»“æœï¼Œå¼€å¯ 5 çº¿ç¨‹å¹¶å‘å¤„ç†...\n")

        # === å¹¶å‘å¤„ç†æ ¸å¿ƒ ===
        with ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤ä»»åŠ¡
            future_to_item = {
                executor.submit(process_single_search_result, idx, item): item 
                for idx, item in enumerate(results_list)
            }
            
            # è·å–ç»“æœ
            for future in as_completed(future_to_item):
                try:
                    data = future.result()
                    if data:
                        final_results.append(data)
                except Exception as exc:
                    print(f"çº¿ç¨‹æ‰§è¡Œå¼‚å¸¸: {exc}")

        print(f"\nğŸ‰ æµç¨‹ç»“æŸï¼Œæœ‰æ•ˆæ±‡æ€»: {len(final_results)} ç¯‡")
        return final_results
    else:
        print("æœªæ‰¾åˆ°æœç´¢ç»“æœ")
        return []
# python search_service.py
if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    res = search_for_keyword("Cè¯­è¨€ èºæ—‹çŸ©é˜µä¸Zå­—å½¢éå† ç®—æ³•")
    print(json.dumps(res, indent=2, ensure_ascii=False))