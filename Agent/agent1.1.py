import streamlit as st
import asyncio
import os
import json
import base64
import pdfplumber
import docx
from datetime import datetime
from typing import List, Dict, Any, Tuple, AsyncGenerator
from dotenv import load_dotenv
from openai import AsyncOpenAI
try:
    import search_service
except ImportError:
    st.error("æ‰¾ä¸åˆ° search_service.pyï¼Œè¯·ç¡®ä¿è¯¥æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")



# streamlit run agent1.1.py
# ==========================================
# 1. âš™ï¸ é…ç½®ä¸åˆå§‹åŒ–
# ==========================================
load_dotenv()  #å¯¼å…¥secrets
# nest_asyncio.apply()  # å…è®¸åµŒå¥—äº‹ä»¶å¾ªç¯

PROFILE_PATH = "profile.txt"
HISTORY_PATH = "history.json"

API_KEY = st.secrets["API_KEY"]
BASE_URL = "https://openrouter.ai/api/v1"

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)

# ã€é‡è¦é…ç½®ã€‘æ¨¡å‹è§’è‰²åˆ†é…
# è¿™é‡Œçš„æ¨¡å‹é€‰æ‹©å†³å®šäº†æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€ (Vision)
MODEL_CONFIG = {
    "librarian": "google/gemini-3-flash-preview", 
    "reviewer": "google/gemini-3-flash-preview",
    "architect": "google/gemini-3-flash-preview",
    "mentor": "anthropic/claude-opus-4.5"          
}

# ==========================================
# 2. ğŸ› ï¸ æ ¸å¿ƒå·¥å…·å‡½æ•° (Utils)
# ==========================================

def encode_image_to_base64(image_bytes: bytes) -> str:
    """
    [å·¥å…·] å°†å›¾ç‰‡äºŒè¿›åˆ¶æµè½¬æ¢ä¸º Base64 å­—ç¬¦ä¸²ã€‚
    ç”¨äºå°†å›¾ç‰‡ä¼ ç»™æ”¯æŒ Vision çš„ LLMã€‚
    """
    return base64.b64encode(image_bytes).decode('utf-8')

def parse_uploaded_file(uploaded_file) -> Dict[str, Any]:
    """
    [æ ¸å¿ƒå·¥å…·] é€šç”¨æ–‡ä»¶è§£æå·¥å‚ã€‚
    è¾“å…¥: Streamlit ä¸Šä¼ æ–‡ä»¶å¯¹è±¡
    è¾“å‡º: å­—å…¸ {'filename':..., 'type': 'code'/'document'/'image'/'error', 'content':...}
    """
    file_type = uploaded_file.name.split('.')[-1].lower()
    result = {
        "filename": uploaded_file.name,
        "type": "unknown",
        "content": ""
    }

    try:
        if file_type == 'pdf':
            with pdfplumber.open(uploaded_file) as pdf:
                text_parts = []
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:  # âœ… æ£€æŸ¥æ˜¯å¦ä¸º None
                        text_parts.append(page_text)
                text = '\n'.join(text_parts)
            result["type"] = "document"
            result["content"] = text if text else "[PDF æ— æ³•æå–æ–‡æœ¬ï¼Œå¯èƒ½æ˜¯æ‰«æç‰ˆ]"

        elif file_type == 'docx':
            doc = docx.Document(uploaded_file)
            text = "\n".join(para.text for para in doc.paragraphs)
            result["type"] = "document"
            result["content"] = text

        elif file_type in ['txt', 'c', 'cpp', 'py', 'java', 'md', 'js', 'ts', 'go', 'rs']:
            text = uploaded_file.read().decode("utf-8", errors='ignore')
            result["type"] = "code"
            result["content"] = text

        elif file_type in ['png', 'jpg', 'jpeg', 'gif', 'webp']:
            bytes_data = uploaded_file.getvalue()
            result["type"] = "image"
            result["content"] = encode_image_to_base64(bytes_data)

    except Exception as e:
        result["type"] = "error"
        result["content"] = f"è¯»å–æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {str(e)}"

    return result



async def search_web_tool(query: str) -> str:
    """
    [AI æ¥å£] ç»Ÿä¸€å°è£…çš„ LLM è°ƒç”¨å‡½æ•° (Generator)ã€‚
    """
    print(f"ğŸ” [Agent] æ­£åœ¨è°ƒç”¨æœç´¢å·¥å…·ï¼Œå…³é”®è¯: {query} ...")
    
    try:
        raw_results = await asyncio.to_thread(search_service.search_for_keyword, query)
        
        if not raw_results:
            return f"ã€æœç´¢ç»“æœã€‘å…³äº '{query}' æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç½‘ç»œä¿¡æ¯ã€‚"

        # å°† JSON å¯¹è±¡æ‹¼æ¥æˆæ¸…æ™°çš„æ–‡æœ¬æŠ¥å‘Šä¾› LLM é˜…è¯»
        formatted_report = f"ä»¥ä¸‹æ˜¯å…³äº '{query}' çš„è”ç½‘æœç´¢ç»“æœæ±‡æ€»ï¼š\n\n"
        
        for idx, item in enumerate(raw_results):
            # å®¹é”™è·å–å­—æ®µï¼Œé˜²æ­¢æŸäº›å­—æ®µç¼ºå¤±
            title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
            url = item.get('source_url', '#')
            summary = item.get('summary', 'æš‚æ— æ‘˜è¦')
            key_points = item.get('key_points', [])
            code_snippets = item.get('code_snippets', [])

            formatted_report += f"--- æ¥æº [{idx + 1}] : {title} ---\n"
            formatted_report += f"é“¾æ¥: {url}\n"
            formatted_report += f"æ‘˜è¦: {summary}\n"
            
            if key_points:
                formatted_report += "å…³é”®ç‚¹:\n"
                for point in key_points:
                    formatted_report += f"   - {point}\n"
            
            if code_snippets:
                formatted_report += "ç›¸å…³ä»£ç ç‰‡æ®µ:\n"
                for code in code_snippets:
                    formatted_report += f"```\n{code[:1500]}...\n```\n"
            
            formatted_report += "\n"

        return formatted_report

    except Exception as e:
        error_msg = f"æœç´¢å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}"
        print(error_msg)
        return error_msg

async def call_ai_chat(model: str, system_prompt: str, user_content: str, image_base64_list: List[str] = None):
    """
    [AI æ¥å£] ç»Ÿä¸€å°è£…çš„ LLM è°ƒç”¨å‡½æ•°ã€‚
    """
    messages = [
        {"role": "system", "content": system_prompt}
    ]

    if not image_base64_list:
        messages.append({"role": "user", "content": user_content})
    else:
        content_payload = []
        if user_content:
            content_payload.append({"type": "text", "text": user_content})
            
        # æ·»åŠ å›¾ç‰‡
        for img_b64 in image_base64_list:
            content_payload.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{img_b64}" 
                }
            })
            
        messages.append({"role": "user", "content": content_payload})

    try:
        # 3. å‘èµ·å¼‚æ­¥è°ƒç”¨
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.7,
            stream=True 
        )
        async for chunk in response:
            content = chunk.choices[0].delta.content
            if content:
                yield content

    except Exception as e:
        yield f"[Aiè°ƒç”¨å¤±è´¥]:{str(e)}"
# ==========================================
# 3. Agent æ ¸å¿ƒé€»è¾‘ (Agents)
# ==========================================

# --- Phase 1: é¢„å¤„ç† ---

async def agent_librarian(uploaded_files) -> Tuple[Dict[str, Any], str]:
    """
    [Librarian - æ¡£æ¡ˆç®¡ç†å‘˜]
    èŒè´£ï¼šæ¸…æ´—æ•°æ®ï¼Œåˆ†ç±»æ•´ç†ï¼Œä¸è¿›è¡Œæ·±åº¦åˆ†æã€‚
    """
    # TODO:
    # 1. éå† uploaded_files
    # 2. è°ƒç”¨ parse_uploaded_file è§£ææ¯ä¸ªæ–‡ä»¶
    # 3. å°†ç»“æœåˆ†ç±»æ”¾å…¥ list: codes[], docs[], images[]
    # 4. è¿”å›ç»“æ„åŒ–å­—å…¸ structured_context

    #1.context å­—å…¸åµŒåˆ—è¡¨
    context = {"code":[],"docs":[],"images":[]}
    for file in uploaded_files:
        parsed_data = parse_uploaded_file(file)
        if parsed_data['type'] == 'code':
            context['code'].append(parsed_data['content'])
        elif parsed_data['type'] == 'image':
            context["images"].append(parsed_data['content'])
        elif parsed_data["type"] == 'document':
            context["docs"].append(parsed_data['content'])

    current_profile = ""
    if os.path.exists(PROFILE_PATH):
        with open(PROFILE_PATH,"r",encoding="utf-8") as f:
            current_profile = f.read()
    else:
        current_profile = "è¿™æ˜¯ç”¨æˆ·ç¬¬ä¸€å‘¨ï¼Œæš‚æ— ä¸ªäººèƒ½åŠ›æ¡£æ¡ˆã€‚"

    return context,current_profile

async def agent_librarian_write(code_list: List[str]) -> str:
    """
    [Librarian - æ¡£æ¡ˆç®¡ç†å‘˜ (å†™æ“ä½œ)]
    èŒè´£ï¼šç›´æ¥è¯»å–æœ¬åœ°æ—§æ¡£æ¡ˆï¼Œå¹¶ç»“åˆæœ¬å‘¨ä¸Šä¼ çš„ã€åŸå§‹ä»£ç ã€‘ï¼Œæ›´æ–° profile.txtã€‚
    å‚æ•°ï¼šcode_list (åŒ…å«æœ¬å‘¨æ‰€æœ‰ä»£ç æ–‡æœ¬çš„åˆ—è¡¨)
    """   

    # è¯»å–æ—§æ¡£æ¡ˆ
    if os.path.exists(PROFILE_PATH):
        with open(PROFILE_PATH, "r", encoding="utf-8") as f:
            old_profile = f.read()
    else:
        old_profile = "ã€æ–°ç”¨æˆ·ã€‘æš‚æ— å†å²æ¡£æ¡ˆã€‚åˆå§‹è¯„çº§ï¼šæœªå®šã€‚"

    #é¢„å¤„ç†ä»£ç å†…å®¹
    raw_code_content = "\n\n--- Next File ---\n\n".join(code_list)
    # if len(raw_code_content) > 50000: 
    #     raw_code_content = raw_code_content[:50000] + "\n...(ä»£ç è¿‡é•¿å·²æˆªæ–­)..."

    system_prompt = """
    ã€ä»»åŠ¡æŒ‡ä»¤ã€‘
    æ‰§è¡ŒæŠ€æœ¯æ¡£æ¡ˆçš„å¢é‡æ›´æ–°ä»»åŠ¡ã€‚åŸºäºè¾“å…¥çš„[æ—§æ¡£æ¡ˆ]ä¸[æœ¬å‘¨åŸå§‹ä»£ç ]ï¼Œè¾“å‡ºä¸€ä»½**è¯æ®å¯¼å‘**ã€å±‚çº§åˆ†æ˜çš„æœ€æ–°æŠ€æœ¯æ¡£æ¡ˆã€‚

    ã€æ ¸å¿ƒåŸåˆ™ã€‘
    1. **æ‹’ç»ç©ºæ´ (No Vague Terms)**ï¼š**ä¸¥ç¦**ä½¿ç”¨â€œæŒæ¡â€ã€â€œç†Ÿç»ƒâ€ã€â€œäº†è§£â€ç­‰ä¸»è§‚å½¢å®¹è¯ã€‚å¿…é¡»ç”¨â€œ**åŠ¨ä½œ+ç»“æœ**â€çš„å½¢å¼æè¿°ã€‚
    2. **è¯æ®å¼ºåˆ¶ (Evidence-Based)**ï¼šæ¯ä¸€é¡¹æŠ€æœ¯èƒ½åŠ›**å¿…é¡»**é™„å¸¦ç®€çŸ­çš„ä¾‹å­ï¼ˆä»£ç ä¸­çš„å…·ä½“åº”ç”¨åœºæ™¯ã€å‡½æ•°åæˆ–è§£å†³çš„é—®é¢˜ï¼‰ä½œä¸ºä½è¯ã€‚
    3. **å¢é‡ä¿ç•™**ï¼šä¿ç•™[æ—§æ¡£æ¡ˆ]æ‰€æœ‰æ¡ç›®ã€‚ä»…åœ¨æœ‰æ–°è¯æ®æ—¶è¿½åŠ å†…å®¹ï¼Œ**ä¸¥ç¦åˆ é™¤**å†å²è®°å½•ã€‚

    ã€è¾“å‡ºæ ¼å¼è§„èŒƒã€‘
    æ¡£æ¡ˆå¿…é¡»æŒ‰ä»¥ä¸‹ Markdown å±‚çº§ç»“æ„ç»„ç»‡ï¼š

    # I. [æŠ€æœ¯æ ˆå¤§ç±»](ä¾‹å¦‚ï¼šPythonã€å‰ç«¯æŠ€æœ¯ã€DevOpsã€C/C++ç®—æ³•ã€Java)
    ## [åºå·].[å­åºå·] [å…·ä½“é¢†åŸŸ/åº“]
    - [åŠ¨è¯+æŠ€æœ¯ç‚¹]ï¼š[ç®€çŸ­çš„ä»£ç è¯æ®/åº”ç”¨åœºæ™¯]
    - [åŠ¨è¯+æŠ€æœ¯ç‚¹]ï¼š[ç®€çŸ­çš„ä»£ç è¯æ®/åº”ç”¨åœºæ™¯]

    *âŒ é”™è¯¯å†™æ³•ï¼š*
    - ç†Ÿç»ƒä½¿ç”¨ Python å¼‚æ­¥ç¼–ç¨‹
    - æŒæ¡ Streamlit

    *âœ… æ­£ç¡®å†™æ³•ç¤ºä¾‹ï¼š*
    # I. Python
    ## 1.1 å¹¶å‘ä¸å¼‚æ­¥ IO
    - **å®ç°å¹¶å‘ä»»åŠ¡è°ƒåº¦**ï¼šåœ¨ `agent_reviewer` æ¨¡å—ä¸­ä½¿ç”¨ `asyncio.gather` å¹¶è¡Œæ‰§è¡Œæœç´¢ä»»åŠ¡ï¼Œæå‡å“åº”é€Ÿåº¦ã€‚
    - **ä¼˜åŒ–å†…å­˜å ç”¨**ï¼šåˆ©ç”¨ `yield` ç”Ÿæˆå™¨æ„å»ºæµå¼æ•°æ®å¤„ç†ç®¡é“ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤§æ–‡ä»¶ã€‚

    # II. Web å…¨æ ˆ
    ## 2.1 Streamlit æ¡†æ¶
    - **æ„å»ºçŠ¶æ€ç®¡ç†æœºåˆ¶**ï¼šä½¿ç”¨ `st.session_state` è·¨é¡µé¢/è·¨åˆ·æ–°æŒä¹…åŒ–å­˜å‚¨ç”¨æˆ·å¯¹è¯å†å²ã€‚
    - **å¼€å‘è‡ªå®šä¹‰ç»„ä»¶**ï¼šå°è£… `parse_uploaded_file` å‡½æ•°å·¥å‚ï¼Œç»Ÿä¸€å¤„ç† PDF/Docx/Code å¤šæ ¼å¼æ–‡ä»¶è§£æã€‚

    # III. C/C++ç®—æ³•
    ## 3.1 .....

    ã€æ‰§è¡Œæ­¥éª¤ã€‘
    1. **è¯æ®æå–**ï¼šæ‰«æä»£ç ï¼Œè¯†åˆ«æŠ€æœ¯ç‚¹ï¼Œå¹¶ç«‹åˆ»æ‰¾åˆ°å®ƒåœ¨ä»£ç ä¸­â€œå…·ä½“è§£å†³äº†ä»€ä¹ˆé—®é¢˜â€æˆ–â€œå…·ä½“å®ç°åœ¨å“ªé‡Œâ€ã€‚
    2. **åŠ¨ä½œåŒ–æè¿°**ï¼šå°†â€œä½¿ç”¨äº† X åº“â€è½¬åŒ–ä¸ºâ€œåˆ©ç”¨ X åº“å®ç°äº† Y åŠŸèƒ½â€ã€‚
    3. **å¢é‡å†™å…¥**ï¼šå°†æ–°å‘ç°çš„èƒ½åŠ›æ¡ç›®è¿½åŠ åˆ°å¯¹åº”åˆ†ç±»ä¸‹ï¼Œè¾“å‡ºå®Œæ•´çš„æ¡£æ¡ˆã€‚
    """
    
    user_content = f"ã€å½“å‰æ—§æ¡£æ¡ˆã€‘:\n{old_profile}\n\nã€æœ¬å‘¨åŸå§‹ä»£ç å † (Raw Code Data)ã€‘:\n{raw_code_content}"

    model = MODEL_CONFIG["librarian"]

    new_profile_content = ""
    
    #è°ƒç”¨ AI ç”Ÿæˆæ–°æ¡£æ¡ˆ
    try:
        async for chunk in call_ai_chat(model, system_prompt, user_content):
            new_profile_content += chunk
    except Exception as e:
        return f"[æ¡£æ¡ˆæ›´æ–°å¤±è´¥]: {str(e)}"
    
    #å†™å…¥æ–‡ä»¶ (è¦†ç›–æ›´æ–°)
    try:
        with open(PROFILE_PATH, "w", encoding="utf-8") as f:
            f.write(new_profile_content)
        print("[Librarian] profile.txt å·²æ ¹æ®åŸå§‹ä»£ç æ›´æ–°å®Œæ¯•ã€‚")
    except Exception as e:
        return f"[æ–‡ä»¶å†™å…¥é”™è¯¯]: {str(e)}"

    return new_profile_content

async def agent_reviewer(context: Dict) -> AsyncGenerator[str, None]:
    """
    [Reviewer - ä»£ç å®¡è®¡å‘˜]
    æ¶æ„å‡çº§ï¼šPlanner (ç”Ÿæˆæœç´¢è¯) -> Executor (å¹¶è¡Œæœç´¢) -> Generator (æµå¼äº§å‡º)
    """
    code_snippets = context.get('code',[])
    image_list = context.get('images',[])
    full_code_text = "\n\n".join(code_snippets)
    if len(full_code_text) > 30000:
        full_code_text = full_code_text[:30000] + "\n\n(ä»£ç è¿‡é•¿ï¼Œåç»­éƒ¨åˆ†å·²æˆªæ–­...)"
    if not full_code_text and not image_list:
        yield "[å®¡è®¡å‘˜]ï¼šæœªæ£€æµ‹åˆ°æœ‰æ•ˆä»£ç æˆ–æˆªå›¾ï¼Œæ— æ³•æ‰§è¡Œå®¡è®¡ã€‚"
        return
    yield "ğŸ¤” **[AI æ€è€ƒä¸­]** æ­£åœ¨åˆ†æä»£ç æŠ€æœ¯æ ˆï¼Œè§„åˆ’æœç´¢è·¯å¾„...\n\n"

    planner_prompt = """
    ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯å®¡è®¡è§„åˆ’å¸ˆã€‚è¯·åˆ†æç”¨æˆ·çš„ä»£ç ï¼Œæå–å‡ºæ€»å…± 3 ä¸ªæœ€é‡è¦çš„çš„æœ€æ–°çš„æŠ€æœ¯å…³é”®è¯æˆ–çŸ¥è¯†ç‚¹ï¼Œç”¨äºåç»­çš„è”ç½‘æœç´¢ä»¥è·å–ç›¸å…³èµ„æ–™ã€‚
    
    ã€æœç´¢ç›®çš„ã€‘
    1. æŸ¥æ‰¾ä»£ç æ‰€ç”¨æ¡†æ¶ï¼ˆå¦‚ Streamlit, LangChain, PyTorch ç­‰ï¼‰çš„æœ€æ–°**å®˜æ–¹æ–‡æ¡£**ã€‚
    2. æŸ¥æ‰¾é’ˆå¯¹å½“å‰ä»£ç é€»è¾‘çš„**æœ€ä½³å­¦ä¹ å†…å®¹**æˆ–**æœ€æ–°æ ‡å‡†å†™æ³•**ã€‚
    3. æŸ¥æ‰¾ä¸ä»£ç éš¾åº¦æˆ–è€…çŸ¥è¯†ç‚¹åŒ¹é…çš„**ç»ƒä¹ é¢˜**ï¼ˆLeetCode/Kaggle/GitHubï¼‰ã€‚

    ã€è¾“å‡ºæ ¼å¼ã€‘
    å¿…é¡»ä¸”ä»…è¾“å‡ºä¸€ä¸ª Python åˆ—è¡¨æ ¼å¼çš„å­—ç¬¦ä¸²(åŠ ä¸Šæ˜æ˜¾çš„åç¼€ï¼Œå¦‚"é¢˜ç›®" "å®˜æ–¹æ–‡æ¡£")ï¼Œä¾‹å¦‚ï¼š
    ["Streamlit å®˜æ–¹æ–‡æ¡£", "Python asyncio é¢˜ç›®", "RAG system GitHubé¡¹ç›®"]
    """
    planner_model = MODEL_CONFIG["reviewer"]
    search_queries = []

    try:
        # è¿™é‡Œæˆ‘ä»¬ä¸æµå¼ï¼Œç›´æ¥æ‹¿åˆ°å®Œæ•´ç»“æœ
        planner_response = ""
        async for chunk in call_ai_chat(planner_model, planner_prompt, f"ã€ä»£ç å†…å®¹ã€‘:\n{full_code_text[:10000]}"):
            planner_response += chunk
        
        # æ¸…æ´—å¹¶è§£æ JSON
        clean_json = planner_response.replace("```json", "").replace("```", "").strip()
        search_queries = json.loads(clean_json)
        
        # å®¹é”™ï¼šå¦‚æœ AI è¿”å›çš„ä¸æ˜¯åˆ—è¡¨ï¼Œå¼ºåˆ¶è½¬ä¸ºåˆ—è¡¨
        if not isinstance(search_queries, list):
            search_queries = [str(search_queries)]
            
    except Exception as e:
        # é™çº§ç­–ç•¥ï¼šå¦‚æœè§„åˆ’å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯
        print(f"[Planner Error]: {e}")
        search_queries = ["æœ¬å‘¨æœ€ä½³GitHubå¼€æºé¡¹ç›®"]


    #å¼€å§‹è°ƒç”¨è”ç½‘æœç´¢å·¥å…·
    search_results_context = ""
    if search_queries:
        # å®æ—¶åé¦ˆç»™ç”¨æˆ·æ­£åœ¨æœä»€ä¹ˆ
        yield f"ğŸŒ **[è”ç½‘æ£€ç´¢]** æ­£åœ¨å¹¶è¡Œæœç´¢æƒå¨èµ„æ–™ï¼š\n"
        for q in search_queries:
            yield f"- *æ£€ç´¢ï¼š{q}*\n"
        yield "\n"

        # å¹¶è¡Œæ‰§è¡Œæœç´¢ä»»åŠ¡ (ä½¿ç”¨ asyncio.gather æé€Ÿ)
        try:
            tasks = [search_web_tool(query) for query in search_queries]
            results = await asyncio.gather(*tasks)
            search_results_context = "\n\n".join(results)
        except Exception as e:
            search_results_context = f"æœç´¢è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}"

    yield "ğŸ“ **[ç”ŸæˆæŠ¥å‘Š]** èµ„æ–™æ£€ç´¢å®Œæ¯•ï¼Œæ­£åœ¨æ’°å†™æ·±åº¦å®¡è®¡æŠ¥å‘Š...\n\n---\n\n"

    system_prompt = """
    ã€ä»»åŠ¡å®šä¹‰ã€‘
    ä¾æ®æä¾›çš„[ä»£ç ç‰‡æ®µ]ã€[è¿è¡Œæˆªå›¾]åŠå‰åºæ­¥éª¤è·å–çš„[è”ç½‘å‚è€ƒèµ„æ–™]ï¼Œæ’°å†™ä¸¥æ ¼çš„ä»£ç å®¡è®¡æŠ¥å‘Šã€‚

    ã€è¾“å…¥è¯´æ˜ã€‘
    1. **å¾…å®¡è®¡ä»£ç **ï¼šç”¨æˆ·çš„åŸå§‹ä»£ç ã€‚
    2. **è”ç½‘å‚è€ƒèµ„æ–™**ï¼šç³»ç»Ÿå·²æå‰æ£€ç´¢åˆ°çš„å®˜æ–¹æ–‡æ¡£ã€æœ€ä½³å®è·µæˆ–ç»ƒä¹ é¢˜æ•°æ®ï¼Œè¿™ä¸æ˜¯ç”¨æˆ·çš„æ•°æ®ï¼Œè¿™æ˜¯è”ç½‘æ‰€å¾—æ•°æ®ã€‚

    ã€æ‰§è¡Œæµç¨‹ã€‘
    1. **è§†è§‰è¯Šæ–­ **ï¼šè‹¥åŒ…å«å›¾ç‰‡ï¼ˆæŠ¥é”™/è¿è¡Œæˆªå›¾ï¼‰ï¼Œä¼˜å…ˆè§£æé”™è¯¯ä¿¡æ¯ï¼Œå¹¶å®šä½ä»£ç ä¸­çš„å…·ä½“è‡´é”™è¡Œã€‚
    2. **å®‰å…¨æ‰«æ**ï¼šæ£€æµ‹å…³é”®æ¼æ´ï¼ˆSQLæ³¨å…¥ã€XSSã€ç¡¬ç¼–ç å¯†é’¥ã€æ•æ„Ÿæ•°æ®æ³„éœ²ã€è¶Šæƒè®¿é—®ï¼‰ã€‚
    3. **å¥å£®æ€§è¯„ä¼°**ï¼šè¯†åˆ«è¿è¡Œæ—¶é£é™©ï¼ˆç©ºæŒ‡é’ˆã€æœªæ•è·å¼‚å¸¸ã€æ­»å¾ªç¯ã€èµ„æºæœªå…³é—­ã€è¯­æ³•é”™è¯¯ï¼‰ã€‚
    4. **ä»£ç å¼‚å‘³**ï¼šæŒ‡å‡ºä¸å¯è¯»å‘½åã€é­”æ³•æ•°å­—ã€å†—ä½™é€»è¾‘æˆ–åæ¨¡å¼å†™æ³•ã€‚
    2. **èµ„æ–™æ•´åˆ**ï¼š
    - **éªŒè¯**ï¼šåˆ©ç”¨[è”ç½‘å‚è€ƒèµ„æ–™]æ ¡éªŒä»£ç ä¸­çš„APIç”¨æ³•æˆ–è€…å…¶å®ƒè¾ƒæ–°ç”šè‡³é™Œç”Ÿçš„å†™æ³•æ˜¯å¦è¿‡æ—¶æˆ–é”™è¯¯ã€‚
    - **æ¨è**ï¼šä»[è”ç½‘å‚è€ƒèµ„æ–™]ä¸­æå–é€‚åˆå½“å‰ä»£ç æ°´å¹³çš„**ç»ƒä¹ é¢˜é“¾æ¥**æˆ–**å®˜æ–¹æ–‡æ¡£é“¾æ¥**ã€‚

    ã€è¾“å‡ºæ¿å—ã€‘(Markdown)
    ä»…åŒ…å«ä»¥ä¸‹æ¿å—ï¼ˆæ— å†…å®¹åˆ™çœç•¥ï¼‰ï¼š
    - **ğŸ”´ è‡´å‘½é—®é¢˜**ï¼šå¯¼è‡´å´©æºƒæˆ–ä¸¥é‡å®‰å…¨éšæ‚£çš„é”™è¯¯ï¼ˆç½—åˆ—é”™è¯¯ï¼Œå±•ç¤ºç›¸åº”ä»£ç ç‰‡æ®µï¼Œå¯ä»¥å±•ç¤ºä¿®å¤åçš„ä»£ç ï¼‰ã€‚
    - **ğŸŸ¡ æ”¹è¿›å»ºè®®**ï¼šé€»è¾‘ç®€åŒ–ä¸ä»£ç è§„èŒƒã€‚
    - **ğŸ“¸ æˆªå›¾åˆ†æ**ï¼šé’ˆå¯¹æŠ¥é”™æˆªå›¾çš„æŠ€æœ¯è§£è¯»ã€‚
    - **ğŸ’¡ ä¿®å¤ä»£ç **ï¼šé’ˆå¯¹ä¸¥é‡é—®é¢˜çš„æœ€å°åŒ–ä¿®å¤æ–¹æ¡ˆã€‚
    - **ğŸ“š æ‰©å±•ä¸å‚è€ƒ**ï¼š**å¼ºåˆ¶**åœ¨æ­¤å¤„ç½—åˆ—[è”ç½‘å‚è€ƒèµ„æ–™]ä¸­æä¾›çš„æ ¸å¿ƒé“¾æ¥ï¼ˆå¦‚å®˜æ–¹æ–‡æ¡£URLã€ç»ƒä¹ é¢˜URLï¼‰ï¼Œç„¶åå†è¡¥å……å…¶ä¸­é—æ¼çš„çŸ¥è¯†ç‚¹æ–‡æ¡£é“¾æ¥ã€‚

    ã€é£æ ¼çº¦æŸã€‘
    å®¢è§‚ã€ç›´æ¥ã€‚ä¸¥ç¦å¿½ç•¥æä¾›çš„[è”ç½‘å‚è€ƒèµ„æ–™]ï¼Œä¸¥ç¦è¾“å‡ºå¯’æš„è¯­ã€‚
    """

    user_content_for_review = f"""
    ã€å¾…å®¡è®¡ä»£ç ã€‘:
    {full_code_text}

    ã€è”ç½‘å‚è€ƒèµ„æ–™ (éå®¡è®¡å†…å®¹ï¼Œä¸ºå‚è€ƒå†…å®¹)ã€‘:
    {search_results_context}
    """
    model = MODEL_CONFIG["reviewer"] 
    try:
        async for chunk in call_ai_chat(model, system_prompt, user_content_for_review, image_base64_list=image_list):
            yield chunk

    except Exception as e:
        error_msg = f"\n\n[reviewer è¿è¡Œå‡ºé”™]: {str(e)}"        
        print(error_msg)
        yield error_msg


async def agent_architect(context: Dict) -> AsyncGenerator[str, None]:
    """
    [Architect - æŠ€æœ¯æ¶æ„å¸ˆ]
    èŒè´£ï¼šæ€§èƒ½è¯„ä¼°ã€æŠ€æœ¯æ ˆå¯¹æ¯”ã€æˆé•¿å€¼è®¡ç®—ã€‚
    """
    #è¯»å–æ—§æ¡£æ¡ˆ 
    old_profile = ""
    if os.path.exists(PROFILE_PATH):
        try:
            with open(PROFILE_PATH, "r", encoding="utf-8") as f:
                old_profile = f.read()
        except Exception:
            old_profile = "è¯»å–æ¡£æ¡ˆå‡ºé”™ï¼Œè§†ä¸ºç©ºç™½æ¡£æ¡ˆã€‚"
    else:
        old_profile = "ã€æ–°ç”¨æˆ·ã€‘æš‚æ— å†å²æ¡£æ¡ˆã€‚åˆå§‹è¯„çº§ï¼šæœªå®šã€‚"

    #æå–å¹¶æ¸…æ´—æ•°æ®
    code_snippets = context.get('code', [])
    word_snippets = context.get('docs', [])


    full_code_text = "\n\n".join(code_snippets)
    if len(full_code_text) > 30000:
        full_code_text = full_code_text[:30000] + "\n\n(ä»£ç è¿‡é•¿ï¼Œåç»­éƒ¨åˆ†å·²æˆªæ–­...)"
    
    full_doc_text = "\n\n".join(word_snippets)
    if len(full_doc_text) > 30000:
        full_doc_text = full_doc_text[:30000] + "\n...(æ–‡æ¡£è¿‡é•¿å·²æˆªæ–­)"

    #ç©ºå†…å®¹æ£€æŸ¥
    if not full_code_text and not full_doc_text:
        yield "ã€æ¶æ„å¸ˆã€‘: æœªæ£€æµ‹åˆ°ä»£ç æˆ–æŠ€æœ¯æ–‡æ¡£ï¼Œæ— æ³•è¿›è¡Œæ¶æ„è¯„ä¼°ã€‚"
        return
    
    system_prompt = """
    ã€æŒ‡ä»¤ç›®æ ‡ã€‘
    åŸºäºç”¨æˆ·[æ—§æ¡£æ¡ˆ]ä¸[æœ¬å‘¨ä»£ç /æ–‡æ¡£]ï¼Œæ‰§è¡Œå®è§‚æ¶æ„/ä»£ç è®¾è®¡æ€§èƒ½è¯„ä¼°ä¸æŠ€æœ¯æˆé•¿åˆ¤å®šã€‚å¿½ç•¥å…·ä½“è¯­æ³•é”™è¯¯ï¼Œä¸“æ³¨ä»£ç çš„å¯ç»´æŠ¤æ€§ã€è®¾è®¡é€»è¾‘ã€æŠ€æœ¯ä¸Šé™ä¸è¿è¡Œæ•ˆç‡ï¼ˆé‡ç‚¹ï¼Œçœ‹æ¶æ„ï¼Œçœ‹æ—¶é—´å¤æ‚åº¦ï¼‰ã€‚

    ã€æ‰§è¡Œæ­¥éª¤ã€‘
    1. **æ¶æ„åˆ†æ**ï¼šæå–ä»£ç çš„ç»“æ„æ¨¡å¼ï¼ˆå¦‚åˆ†å±‚ã€æ¨¡å—åŒ–ç¨‹åº¦ï¼‰ã€‚è¯†åˆ«æ˜¯å¦åº”ç”¨äº†ç‰¹å®šè®¾è®¡æ¨¡å¼ï¼ˆOOPã€FPã€å•ä¾‹ã€å·¥å‚ç­‰ï¼‰ï¼Œæ˜¯å¦ä¼˜åŒ–äº†è¿è¡Œæ•ˆç‡ï¼Œè¯„ä¼°æ¶æ„æ€§èƒ½ã€‚
    2. **æˆé•¿æ¯”å¯¹**ï¼šå°†æœ¬å‘¨ä»£ç çš„æŠ€æœ¯æ·±åº¦ä¸[æ—§æ¡£æ¡ˆ]è¿›è¡Œå¯¹æ¯”ã€‚
    - åˆ¤å®šçŠ¶æ€ï¼š**çªç ´**ï¼ˆåº”ç”¨äº†æ–°æ¦‚å¿µ/æ–°æŠ€æœ¯/æ–°ç®—æ³•ï¼‰ã€**å·©å›º**ï¼ˆç†Ÿç»ƒåº¦æå‡ï¼‰æˆ– **åœæ»**ã€‚
    3. **æŠ€æœ¯æ ˆæå–**ï¼šç½—åˆ—ä»£ç ä¸­ä½¿ç”¨çš„æ ¸å¿ƒæ¡†æ¶ã€ç¬¬ä¸‰æ–¹åº“æˆ–æ ¸å¿ƒç®—æ³•ã€‚
    4. **ç»¼åˆå®šçº§**ï¼šæ ¹æ®ä»£ç çš„å·¥ç¨‹å¤æ‚åº¦ä¸è®¾è®¡ç¾æ„Ÿæˆ–è€…è¿è¡Œæ•ˆç‡ï¼Œç»™å‡º S/A/B/C è¯„çº§ã€‚
    5. **ä¸è¶³è¯„ä¼°**ï¼šæ ¹æ®ä»£ç çš„å·¥ç¨‹æ¶æ„æ‰¾å‡ºæ€§èƒ½ä¸æ¶æ„ä¸Šçš„ä¸è¶³ç‚¹ï¼Œå¦‚å¯ç»´æŠ¤æ€§ã€è®¾è®¡é€»è¾‘ã€æŠ€æœ¯ä¸Šé™ä¸è¿è¡Œæ•ˆç‡çš„ä¸è¶³ç‚¹ã€‚
    ã€è¾“å‡ºæ ¼å¼ã€‘
    ä¸¥æ ¼éµå¾ª Markdown æ ¼å¼ï¼Œä»…è¾“å‡ºä»¥ä¸‹å››ä¸ªæ¿å—ï¼š

    - **ğŸ—ï¸ æ¶æ„/ä»£ç è¿è¡Œæ•ˆç‡**ï¼š(ç½—åˆ—ä»£ç ç»“æ„ï¼Œè¿è¡Œæ•ˆç‡ï¼ˆç»ƒä¹ ç®—æ³•å°±çœ‹æ—¶é—´å¤æ‚åº¦ï¼Œæ˜¯é¡¹ç›®å°±çœ‹æ¶æ„çš„æ•ˆç‡ï¼‰åŠæ¨¡å—åˆ’åˆ†ï¼ˆå¦‚æœæ˜¯é¡¹ç›®ï¼‰)
    - **ğŸ“ˆ æˆé•¿è¯„ä¼°**ï¼š(æ˜ç¡®æŒ‡å‡ºä¸æ—§æ¡£æ¡ˆç›¸æ¯”çš„è¿›æ­¥ç‚¹ï¼Œé‡ç‚¹æŒ‡å‡ºä¸è¶³ç‚¹ï¼Œåˆ—å‡ºæ•ˆç‡ä½ä¸‹å†…å®¹å¹¶ä¸”ç»™å‡ºä¼˜åŒ–æ¡ˆä¾‹ã€‚æ­¤éƒ¨åˆ†ä¸ºä¸»è¦éƒ¨åˆ†ï¼Œè¾“å‡ºè´´åˆæœ€å¤§è¾“å‡ºä¸Šé™)
    - **ğŸ› ï¸ æŠ€æœ¯æ ˆä¾¦æµ‹**ï¼š(åˆ—å‡ºæ£€æµ‹åˆ°çš„å…³é”®æŠ€æœ¯/åº“/ç®—æ³•)
    - **âš–ï¸ ç»¼åˆè¯„çº§**ï¼š(ç»™å‡º S/A/B/C è¯„åˆ†å¹¶ç®€è¿°ç†ç”±)
    - **ğŸ› ï¸ æ‰©å±•å‚è€ƒ**ï¼š(äº†è§£æ¶æ„/ç®—æ³•ä¸æˆç†Ÿçš„åœ°æ–¹ï¼Œæ¨èå®˜æ–¹æ–‡æ¡£é˜…è¯»æˆ–è€…å¼€æºé¡¹ç›®æˆ–è€…ä¸çŸ¥è¯†ç‚¹ç›¸å…³çš„é¢˜ç›®)
    """
    user_content = f"ã€å½“å‰æ—§æ¡£æ¡ˆã€‘:\n{old_profile}\n\nã€æœ¬å‘¨åŸå§‹ä»£ç å †ã€‘:\n{full_code_text} \n\nã€æœ¬å‘¨æ–‡æ¡£å†…å®¹ã€‘:\n{full_doc_text}"

    model = MODEL_CONFIG["architect"]

    try:
        async for chunk in call_ai_chat(model, system_prompt, user_content):
            yield chunk
    except Exception as e:
        error_msg = f"\n\n[Architect è¿è¡Œå‡ºé”™]: {str(e)}"        
        print(error_msg)
        yield error_msg

async def agent_mentor(review_res: str, architect_res: str, user_note: str,context:Dict) -> AsyncGenerator[str, None]:
    """
    [Mentor - å¯¼å¸ˆ]
    èŒè´£ï¼šæ±‡æ€»æŠ¥å‘Šï¼Œç”Ÿæˆæœ€ç»ˆå‘¨æŠ¥ã€‚
    """
    code_snippets = context.get('code', [])
    system_prompt = """
    ã€æŒ‡ä»¤ç›®æ ‡ã€‘
    åŸºäº[ä»£ç å®¡è®¡æŠ¥å‘Š]ã€[æ¶æ„è¯„ä¼°æŠ¥å‘Š]ã€[å­¦ç”Ÿå¿ƒå¾—]åŠ[å­¦ç”Ÿæºä»£ç ]ï¼Œæ’°å†™ä¸€ä»½ç»¼åˆæ€§çš„ã€Šæœ¬å‘¨æˆé•¿å‘¨æŠ¥ã€‹ã€‚éœ€æ•´åˆå¤šæ–¹ä¿¡æ¯ï¼Œæç‚¼æ ¸å¿ƒè§‚ç‚¹ï¼Œé¿å…å•çº¯å¤è¿°ï¼Œè¦æ±‚çŸ¥è¯†å¯†åº¦é«˜ã€‚

    ã€æ‰§è¡Œé€»è¾‘ã€‘
    1. **æç‚¼é«˜å…‰ (Highlights)**ï¼šä¾æ®æ¶æ„ä¸æ€§èƒ½è¯„ä¼°ï¼Œè¯†åˆ«ä»£ç ä¸­çš„äº®ç‚¹ã€æ°´å¹³æˆ–ç›¸å¯¹äºæ—§æ¡£æ¡ˆçš„æŠ€æœ¯çªç ´ï¼Œä»¥åŠæ‰¾å‡ºä¼˜åŒ–æç¤ºç‚¹ï¼ˆå¦‚å¯ä»¥è¿ç”¨æ›´åŠ é«˜æ•ˆçš„ç®—æ³•æˆ–è€…æ¶æ„ï¼‰ã€‚
    2. **èšç„¦æ”¹è¿› (Focus Area)**ï¼šä»å®¡è®¡æŠ¥å‘Šä¸­ç­›é€‰å‡ºä¼˜å…ˆçº§é«˜çš„ 2-3 ä¸ªé—®é¢˜ï¼ˆå¦‚ä¸¥é‡å®‰å…¨æ¼æ´ã€æ ¸å¿ƒé€»è¾‘è°¬è¯¯æˆ–æ¶åŠ£çš„ç¼–ç ä¹ æƒ¯ï¼‰ï¼Œä½œä¸ºæœ¬å‘¨æ•´æ”¹é‡ç‚¹ã€‚
    3. **å…¨é‡çº é”™ (Error Analysis)**ï¼šç»¼åˆå®¡è®¡å‘˜ä¸æ¶æ„å¸ˆçš„å‘ç°ï¼Œå¹¶ç»“åˆä½ å¯¹åŸå§‹ä»£ç çš„å®¡æŸ¥ï¼Œç½—åˆ—ä»£ç ä¸­å­˜åœ¨çš„é€»è¾‘é”™è¯¯ä¸æ€§èƒ½ä½ä¸‹çš„ç‰‡æ®µã€‚
    4. **ç­”ç–‘ (Q&A)**ï¼šè‹¥[å­¦ç”Ÿå¿ƒå¾—]ä¸­åŒ…å«å…·ä½“æŠ€æœ¯å›°æƒ‘æˆ–æé—®ï¼Œæä¾›ç®€æ˜è§£ç­”ï¼›è‹¥æ— æé—®ï¼Œåˆ™è·³è¿‡æ­¤æ­¥éª¤ã€‚
    5. **è§„åˆ’ä¸‹ä¸€æ­¥ (Next Step)**ï¼šé’ˆå¯¹æœ¬å‘¨æš´éœ²çš„çŸ­æ¿ï¼Œå¸ƒç½®å…·ä½“çš„ä¸“é¡¹è®­ç»ƒé¢˜ç›®æˆ–æ¨èå­¦ä¹ å†…å®¹ï¼ˆé¢˜ç›®ï¼Œæˆ–è€…å®˜æ–¹æ–‡æ¡£ï¼‰ï¼Œå¯ä»¥å‚è€ƒã€ä»£ç å®¡è®¡æŠ¥å‘Šã€‘ä¸­çš„é“¾æ¥ä¸çŸ¥è¯†ç‚¹ã€‚

    ã€è¾“å‡ºæ ¼å¼ã€‘
    ä¸¥æ ¼éµå¾ª Markdown æ ¼å¼ï¼Œè¯­æ°”ä¸“ä¸šä¸”å…·æœ‰æŒ‡å¯¼æ€§ï¼ŒåŒ…å«ä»¥ä¸‹æ¿å—ï¼š
    - ** æœ¬å‘¨é«˜å…‰**
    - ** æ•ˆç‡æ”¹è¿›** (æŒ‡å‡ºéƒ¨åˆ†å¯èƒ½å¯¼è‡´ä»£ç æ•ˆç‡ä½ä¸‹çš„åœ°æ–¹ï¼Œåˆ—å‡ºç”¨æˆ·çš„ä»£ç ï¼ˆæ•ˆç‡ä½ä¸‹çš„ä»£ç ï¼Œæ¯”å¦‚æ—¶é—´å¤æ‚åº¦çˆ†ç‚¸çš„ä»£ç ï¼‰ä¸ä¼˜åŒ–åçš„ä»£ç ï¼ˆæ•ˆç‡é«˜çš„è§£æ³•ä¸ç®—æ³•ï¼‰)(ä¸»è¦éƒ¨åˆ†ï¼Œè¦æ±‚è¾“å‡ºé•¿ï¼Œå°½é‡è´´è¿‘è¾“å‡ºä¸Šé™)
    - ** é”™è¯¯æ¸…å•** (æŒ‡å‡ºæ‰€æœ‰å…·ä½“é”™è¯¯ï¼Œåˆ—å‡ºç”¨æˆ·é”™è¯¯çš„ä»£ç ä¸ä¿®æ­£åçš„ä»£ç ï¼ˆæ ¹æ®æƒ…å†µæä¾›å¤šç§è§£æ³•ï¼‰)(ä¸»è¦éƒ¨åˆ†ï¼Œè¦æ±‚è¾“å‡ºé•¿ï¼Œå°½é‡è´´è¿‘è¾“å‡ºä¸Šé™)
    - ** ç­”ç–‘è§£æƒ‘** (è‹¥æ— é—®é¢˜åˆ™çœç•¥)
    - ** è‡ªèº«å¼ºåŒ–** (ç»™å‡ºç”¨æˆ·ä¸‹ä¸€å‘¨å¯ä»¥å»å­¦ä¹ çš„éƒ¨åˆ†ï¼Œæ¯”å¦‚å»çœ‹...å®˜æ–¹æ–‡æ¡£ï¼Œå»åˆ·...çš„é¢˜ç›®ï¼Œç»™å‡ºé“¾æ¥ï¼ˆå¯ä»¥å‚è€ƒã€ä»£ç å®¡è®¡æŠ¥å‘Šã€‘ä¸­çš„é“¾æ¥ï¼ˆæŠ¥å‘Šä¸­é“¾æ¥è¾ƒæ–°ï¼‰ï¼Œä¹Ÿå¯ä»¥æ ¹æ®ä½ çš„çŸ¥è¯†åº“ï¼‰)
    """

    user_content = user_content = f"""
    ã€å­¦ç”Ÿå¿ƒå¾—ã€‘: {user_note}
    
    ã€ä»£ç å®¡è®¡æŠ¥å‘Šã€‘:
    {review_res}
    
    ã€æ¶æ„è¯„ä¼°æŠ¥å‘Šã€‘:
    {architect_res}
    
    ã€ä»£ç ç‰‡æ®µæ‘˜è¦ã€‘:
    {code_snippets}
    """

    model = MODEL_CONFIG["mentor"]
    try:
        async for chunk in call_ai_chat(model,system_prompt,user_content):
            yield chunk
    except Exception as e:
        error_msg = f"\n\n[Architect è¿è¡Œå‡ºé”™]: {str(e)}"        
        print(error_msg)
        yield error_msg

async def agent_chat(user_query: str):
    """
    [Chat Agent - éšèº«å¯¼å¸ˆ]
    å¤„ç†å¤šè½®å¯¹è¯ï¼Œè‡ªåŠ¨è¯†åˆ«å½“å‰æ˜¯â€œåœºæ™¯A(å¸¦ä»£ç )â€è¿˜æ˜¯â€œåœºæ™¯B(çº¯é—²èŠ)â€ã€‚
    """
    # 1. è·å–å½“å‰æ¡£æ¡ˆ (æ— è®ºå“ªç§åœºæ™¯éƒ½éœ€è¦æ¡£æ¡ˆ)
    current_profile = "æš‚æ— æ¡£æ¡ˆ"
    if os.path.exists(PROFILE_PATH):
        with open(PROFILE_PATH, "r", encoding="utf-8") as f:
            current_profile = f.read()

    # 2. åˆ¤æ–­åœºæ™¯
    context_data = st.session_state.current_context
    analysis_res = st.session_state.analysis_result
    
    system_prompt = ""
    user_context_block = ""

    # === åœºæ™¯ A: åˆšåˆšç»“æŸåˆ†æï¼Œæœ‰ä»£ç å’ŒæŠ¥å‘Š ===
    if context_data and analysis_res:
        code_text = "\n".join(context_data.get('code', []))[:20000] # æˆªæ–­é˜²æº¢å‡º
        mentor_report = analysis_res.get('mentor', '')
        
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½ä¸¥å‰ä½†å¾ªå¾ªå–„è¯±çš„ç¼–ç¨‹å¯¼å¸ˆã€‚ä½ åˆšåˆšå®Œæˆäº†å¯¹è¯¥å­¦ç”Ÿä»£ç çš„å‘¨æŠ¥åˆ†æã€‚
        
        ã€ä½ çš„ä¸»è¦ä¾æ®ã€‘
        1. **å­¦ç”Ÿæ¡£æ¡ˆ**: {current_profile}
        2. **åˆšåˆšåˆ†æçš„ä»£ç **: (è§ä¸‹æ–‡)
        3. **ä½ ç»™å‡ºçš„å‘¨æŠ¥**: (è§ä¸‹æ–‡)
        
        ã€å›å¤ç­–ç•¥ã€‘
        - æ—¢ç„¶ä½ æ‰‹é‡Œæœ‰ä»£ç ï¼Œå½“å­¦ç”Ÿæé—®æ—¶ï¼Œ**å¿…é¡»å¼•ç”¨å…·ä½“ä»£ç è¡Œæ•°**æ¥è§£é‡Šã€‚
        - ç»“åˆä½ åˆšæ‰æŒ‡å‡ºçš„é”™è¯¯æ¸…å•è¿›è¡Œå›ç­”ã€‚
        - ä¿æŒå¤šè½®å¯¹è¯çš„è¿è´¯æ€§ï¼Œä¸è¦é‡å¤è‡ªæˆ‘ä»‹ç»ã€‚
        """
        
        user_context_block = f"""
        ã€å½“å‰ä»£ç ä¸Šä¸‹æ–‡ã€‘:
        {code_text}
        
        ã€ä½ åˆšåˆšç”Ÿæˆçš„å‘¨æŠ¥ã€‘:
        {mentor_report}
        """

    # === åœºæ™¯ B: åˆ·æ–°å/æ— ä»£ç ï¼Œåªæœ‰æ¡£æ¡ˆ ===
    else:
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½ç¼–ç¨‹å¯¼å¸ˆã€‚ç›®å‰æ²¡æœ‰å…·ä½“çš„ä»£ç ä¸Šä¸‹æ–‡ï¼Œä½†ä½ äº†è§£è¿™ä½å­¦ç”Ÿçš„å†å²èƒ½åŠ›ã€‚
        
        ã€å­¦ç”Ÿæ¡£æ¡ˆã€‘: 
        {current_profile}
        
        ã€å›å¤ç­–ç•¥ã€‘
        - å›ç­”å…³äºç¼–ç¨‹ã€èŒä¸šè§„åˆ’æˆ–æŠ€æœ¯æ¦‚å¿µçš„é€šç”¨é—®é¢˜ã€‚
        - å¦‚æœå­¦ç”Ÿé—®å…·ä½“çš„ä»£ç ç»†èŠ‚ï¼Œè¯·ç¤¼è²Œåœ°å‘ŠçŸ¥éœ€è¦å…ˆä¸Šä¼ ä»£ç è¿›è¡Œåˆ†æã€‚
        - æ ¹æ®æ¡£æ¡ˆä¸­çš„â€œå½“å‰å¼±ç‚¹â€æä¾›é’ˆå¯¹æ€§çš„å»ºè®®ã€‚
        """
        
        user_context_block = "ã€å½“å‰çŠ¶æ€ã€‘: æ— ä»£ç ä¸Šä¸‹æ–‡ï¼Œä»…åŸºäºæ¡£æ¡ˆäº¤æµã€‚"

    # 3. æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡ (ä¸ºäº†è®©AIç”±è®°å¿†)
    # å°†æœ€è¿‘15 è½®å¯¹è¯æ‹¼æ¥æˆæ–‡æœ¬ä¼ ç»™ AIï¼Œæ¨¡æ‹Ÿè®°å¿†
    history_text = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.chat_history[-15:]])
    
    final_user_content = f"""
    {user_context_block}
    
    ã€å†å²å¯¹è¯å›é¡¾ã€‘:
    {history_text}
    
    ã€å­¦ç”Ÿå½“å‰æé—®ã€‘: 
    {user_query}
    """

    model = MODEL_CONFIG["reviewer"]

    async for chunk in call_ai_chat(model, system_prompt, final_user_content):
        yield chunk

# ==========================================
# 4. ä¸»å·¥ä½œæµæ§åˆ¶ (Workflow)
# ==========================================

# async def run_weekly_analysis(uploaded_files, user_note, current_profile):

    
async def main():
    # 1. å¿…é¡»æœ€å…ˆæ‰§è¡Œé…ç½®
    st.set_page_config(page_title="AI Coding Mentor", layout="wide", page_icon="ğŸ§™â€â™‚ï¸")
    
    # 2. CSS æ ·å¼ä¼˜åŒ–
    st.markdown("""
    <style>
    .stTextArea textarea { font-size: 16px; }
    div[data-testid="stExpander"] details summary p { font-size: 1.1rem; font-weight: 600; }
    </style>
    """, unsafe_allow_html=True)


    if "analysis_result" not in st.session_state:
        st.session_state.analysis_result = None  

    #ç”¨äºå­˜å‚¨å¤šè½®å¯¹è¯å†å²(åˆ·æ–°å)
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
        
    # ç”¨äºæŒä¹…åŒ–å­˜å‚¨è§£æåçš„ä»£ç å’Œæ–‡æ¡£å†…å®¹
    if "current_context" not in st.session_state:
        st.session_state.current_context = None

    # --- ä¾§è¾¹æ  ---
    with st.sidebar:
        st.header("ğŸ§™â€â™‚ï¸ ä¸ªäººæ¡£æ¡ˆ")
        if os.path.exists(PROFILE_PATH):
            with open(PROFILE_PATH, "r", encoding="utf-8") as f:
                profile_content = f.read()
                with st.expander("ğŸ“œ ç‚¹å‡»æŸ¥çœ‹å®Œæ•´æ¡£æ¡ˆ", expanded=False):
                    st.markdown(profile_content)
        else:
            st.warning("æš‚æ— æ¡£æ¡ˆï¼Œè¯·å…ˆè¿›è¡Œä¸€æ¬¡å‘¨æŠ¥åˆ†æã€‚")

        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰æ•°æ®"):
            if os.path.exists(PROFILE_PATH): os.remove(PROFILE_PATH)
            if os.path.exists(HISTORY_PATH): os.remove(HISTORY_PATH)
            st.session_state.analysis_result = None
            st.rerun()

    # --- ä¸»ç•Œé¢ ---
    st.title("AI Coding Mentor")
    st.caption("ä½ çš„ç§äººæŠ€æœ¯æˆé•¿é¡¾é—®å›¢é˜Ÿ")

    # ä½¿ç”¨ Tabs åˆ†ç¦»å·¥ä½œå°ä¸å†å²
    tab_analysis, tab_chat, tab_history = st.tabs(["ğŸš€ æœ¬å‘¨åˆ†æ", "ğŸ’¬ å¯¼å¸ˆå¯¹è¯", "ğŸ“œ å†å²æ¡£æ¡ˆ"])
    # ==========================
    # Tab 1: åˆ†æå·¥ä½œå°
    # ==========================
    with tab_analysis:
        col_input, col_note = st.columns([1, 1])
        with col_input:
            uploaded_files = st.file_uploader("1. ä¸Šä¼ ä»£ç /æ–‡æ¡£", accept_multiple_files=True)
        with col_note:
            user_note = st.text_area("2. æœ¬å‘¨å¿ƒå¾—", height=100, placeholder="ä¾‹å¦‚ï¼šè¿™å‘¨ä¸»è¦å­¦ä¹ äº†...")

        # ã€ä¿®æ”¹ç‚¹ 1ã€‘åœ¨è¿™é‡Œåˆ›å»ºä¸€ä¸ªç©ºçš„å®¹å™¨å ä½ç¬¦ï¼Œä½ç½®åœ¨æŒ‰é’®ä¸Šæ–¹
        status_placeholder = st.empty()

        start_btn = st.button("å¯åŠ¨å‘¨æŠ¥åˆ†æ", type="primary", use_container_width=True)
        st.divider()

        # é¢„å…ˆå®šä¹‰å¸ƒå±€å®¹å™¨ï¼ˆé˜²æ­¢UIè·³åŠ¨ï¼‰
        st.subheader("ç¬¬ä¸€é˜¶æ®µï¼šæ·±åº¦æŠ€æœ¯è¯„ä¼°")
        col_review, col_arch = st.columns(2)
        with col_review:
            st.markdown("#### ä»£ç å®¡è®¡ (Reviewer)")
            # ä½¿ç”¨ container å›ºå®šé«˜åº¦ï¼Œç¾è§‚
            review_box = st.container(height=500, border=True)
            review_placeholder = review_box.empty()
        
        with col_arch:
            st.markdown("#### æ¶æ„è¯„ä¼° (Architect)")
            arch_box = st.container(height=500, border=True)
            arch_placeholder = arch_box.empty()

        st.subheader("ç¬¬äºŒé˜¶æ®µï¼šå¯¼å¸ˆæ€»ç»“ (Mentor)")
        mentor_box = st.container(border=True)
        mentor_placeholder = mentor_box.empty()

        # --- æ ¸å¿ƒé€»è¾‘ A: ç‚¹å‡»è¿è¡Œ ---
        if start_btn:
            if not uploaded_files:
                st.error("âš ï¸ è¯·å…ˆä¸Šä¼ æ–‡ä»¶ï¼")
            else:
                # ã€ä¿®æ”¹ç‚¹ 2ã€‘æŒ‡å®šåœ¨è¿™ä¸ªå ä½ç¬¦å®¹å™¨å†…æ¸²æŸ“ st.status
                with status_placeholder:
                    # ä½¿ç”¨ st.status æ˜¾ç¤ºè¿›åº¦çŠ¶æ€
                    with st.status("ğŸ”¥ AI å›¢é˜Ÿæ­£åœ¨å¹¶è¡Œå·¥ä½œä¸­...", expanded=True) as status:
                        
                        async def run_async_logic():
                            try:
                                # 1. Librarian
                                st.write("Librarian: æ­£åœ¨æ•´ç†æ–‡ä»¶å¹¶æ›´æ–°æ¡£æ¡ˆ...")
                                context, _ = await agent_librarian(uploaded_files)
                                st.session_state.current_context = context
                                await agent_librarian_write(context['code']) 

                                # 2. Reviewer & Architect å¹¶è¡Œ
                                st.write("Reviewer & Architect: æ­£åœ¨åˆ†æä»£ç ...")
                                
                                # ä¸´æ—¶å­˜å‚¨ç»“æœç”¨äºæ˜¾ç¤º
                                results = {"review": "", "arch": "", "mentor": ""}

                                # å®šä¹‰æµå¼å›è°ƒ
                                async def stream_review():
                                    async for chunk in agent_reviewer(context):
                                        results["review"] += chunk
                                        review_placeholder.markdown(results["review"] + "â–Œ")
                                    review_placeholder.markdown(results["review"])

                                async def stream_arch():
                                    async for chunk in agent_architect(context):
                                        results["arch"] += chunk
                                        arch_placeholder.markdown(results["arch"] + "â–Œ")
                                    arch_placeholder.markdown(results["arch"])

                                await asyncio.gather(stream_review(), stream_arch())

                                # 3. Mentor
                                st.write("Mentor: æ­£åœ¨æ’°å†™å‘¨æŠ¥...")
                                async for chunk in agent_mentor(results["review"], results["arch"], user_note, context):
                                    results["mentor"] += chunk
                                    mentor_placeholder.markdown(results["mentor"] + "â–Œ")
                                mentor_placeholder.markdown(results["mentor"])

                                # 4. ä¿å­˜çŠ¶æ€ä¸æ–‡ä»¶
                                st.session_state.analysis_result = results
                                
                                new_record = {
                                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                                    "note": user_note,
                                    **results 
                                }
                                
                                history = []
                                if os.path.exists(HISTORY_PATH):
                                    try:
                                        with open(HISTORY_PATH, "r", encoding="utf-8") as f:
                                            history = json.load(f)
                                    except: pass
                                
                                history.append(new_record)
                                with open(HISTORY_PATH, "w", encoding="utf-8") as f:
                                    json.dump(history, f, ensure_ascii=False, indent=2)

                                status.update(label="âœ… åˆ†æå®Œæˆï¼å·²å½’æ¡£", state="complete", expanded=False)
                                st.balloons()
                                
                            except Exception as e:
                                st.error(f"è¿è¡Œå‡ºé”™: {e}")

                        
                        await run_async_logic()

        # --- æ ¸å¿ƒé€»è¾‘ B: å›å¡«æ—§æ•°æ® (é˜²æ­¢åˆ·æ–°ç™½å±) ---
        elif st.session_state.analysis_result:
            res = st.session_state.analysis_result
            review_placeholder.markdown(res["review"])
            arch_placeholder.markdown(res["arch"])
            mentor_placeholder.markdown(res["mentor"])

    # ==========================
    # å†å²æ¡£æ¡ˆ
    # ==========================
    with tab_history:
        if not os.path.exists(HISTORY_PATH):
            st.info("ğŸ“­ æš‚æ— å†å²è®°å½•")
        else:
            try:
                with open(HISTORY_PATH, "r", encoding="utf-8") as f:
                    data = json.load(f)
                
                # å€’åºéå†ï¼ˆæœ€æ–°çš„åœ¨æœ€å‰ï¼‰
                for idx, item in enumerate(reversed(data)):
                    ts = item.get('timestamp', 'Unknown')
                    note = item.get('note', '')[:30]
                    
                    with st.expander(f"ğŸ“… {ts} | å¿ƒå¾—: {note}...", expanded=(idx==0)):
                        t1, t2, t3 = st.tabs(["å¯¼å¸ˆå‘¨æŠ¥", "ä»£ç å®¡è®¡", "æ¶æ„è¯„ä¼°"])
                        with t1: st.markdown(item.get('mentor', ''))
                        with t2: st.markdown(item.get('review', ''))
                        with t3: st.markdown(item.get('arch', ''))
            except Exception as e:
                st.error(f"å†å²è®°å½•è¯»å–å¤±è´¥: {e}")
    # ==========================
    # å¯¼å¸ˆå¯¹è¯
    # ==========================
    with tab_chat:
        # 1. é¡¶éƒ¨çŠ¶æ€æç¤º (å¯é€‰ï¼Œæ”¾åœ¨æœ€ä¸Šé¢)
        if st.session_state.current_context:
            st.success("ğŸ§  å·²è¿æ¥ä»£ç å¤§è„‘ï¼šAI å·²è¯»å–ä½ åˆšåˆšæäº¤çš„ä»£ç å’ŒæŠ¥é”™ï¼Œå¯ç›´æ¥æé—®ã€‚")
        else:
            st.info("ğŸ’¬ é—²èŠæ¨¡å¼ï¼šAI ä»…äº†è§£ä½ çš„å†å²æ¡£æ¡ˆï¼Œæ— å½“å‰ä»£ç æ•°æ®ã€‚")

        for msg in st.session_state.chat_history:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        if prompt := st.chat_input("å‘å¯¼å¸ˆæé—® (ä¾‹å¦‚ï¼šè¿™è¡Œä»£ç ä¸ºä»€ä¹ˆæŠ¥é”™ï¼Ÿ)"):
            
            # A. ç”¨æˆ·æé—®ç«‹å³æ˜¾ç¤º (è¿½åŠ åœ¨å†å²è®°å½•ä¸‹æ–¹)
            with st.chat_message("user"):
                st.markdown(prompt)
            # æ›´æ–°å†å²æ•°æ®
            st.session_state.chat_history.append({"role": "user", "content": prompt})

            # B. AI å›å¤ (æµå¼æ˜¾ç¤º)
            with st.chat_message("assistant"):
                response_placeholder = st.empty()
                full_response = ""
                
                # è°ƒç”¨ agent_chat ç”Ÿæˆå›å¤
                async def stream_chat():
                    nonlocal full_response
                    async for chunk in agent_chat(prompt):
                        full_response += chunk
                        response_placeholder.markdown(full_response + "â–Œ")
                    response_placeholder.markdown(full_response)
                
                await stream_chat()
            
            # C. ä¿å­˜ AI å›å¤åˆ°å†å²
            st.session_state.chat_history.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    asyncio.run(main())